{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WalkerBase::__init__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csreid/anaconda3/envs/gpu/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from rebar.learners.adp import ADP\n",
    "from rebar.learners.qlearner import QLearner\n",
    "from rebar.memory import Memory\n",
    "from envs import InvertedPendulum\n",
    "from gym.wrappers import TimeLimit\n",
    "from copy import deepcopy\n",
    "from mutual import HeterogeneousMutualLearner\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "mem = pickle.load(open('dataset.pkl', 'rb'))\n",
    "\n",
    "env = InvertedPendulum\n",
    "play_env = TimeLimit(deepcopy(env), max_episode_steps=1000)\n",
    "eval_env = TimeLimit(deepcopy(env), max_episode_steps=500)\n",
    "play_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp = ADP(\n",
    "    action_space=env.action_space,\n",
    "    observation_space=env.observation_space,\n",
    "    bins=5,\n",
    "    gamma=0.99,\n",
    "    initial_temp=1,\n",
    "    delta=0.01\n",
    ")\n",
    "\n",
    "qlrn = QLearner(\n",
    "    action_space = env.action_space,\n",
    "    observation_space = env.observation_space,\n",
    "    Q = 'simple',\n",
    "    opt_args = { 'lr': 0.01 },\n",
    "    memory_len=1000,\n",
    "    gamma=0.99,\n",
    "    initial_epsilon=1.,\n",
    "    final_epsilon=0.01,\n",
    "    exploration_steps=1000,\n",
    "    target_lag=100\n",
    ")\n",
    "\n",
    "mut = HeterogeneousMutualLearner(\n",
    "    action_space=env.action_space,\n",
    "    observation_space=env.observation_space,\n",
    "    mutual_steps=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_s = [m for m in mem][0][0]\n",
    "\n",
    "qvals = []\n",
    "adpvals = []\n",
    "q_evals = []\n",
    "adp_evals = []\n",
    "\n",
    "for s, a, r, sp, done in mem:\n",
    "    qvals.append(float(torch.max(qlrn.Q(sample_s))))\n",
    "    adpvals.append(np.max(adp.get_action_vals(sample_s)))\n",
    "    #qlrn.handle_transition(s, a, r, sp, done)\n",
    "    #1adp.handle_transition(s, a, r, sp, done)\n",
    "    mut.handle_transition(s, a, r, sp, done)\n",
    "        \n",
    "    #q_evals.append(qlrn.evaluate(eval_env, 5))\n",
    "    #adp_evals.append(adp.evaluate(eval_env, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
